{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import dataclasses\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import custom modules\n",
    "from Model.modules_lstm import LSTMEncoder\n",
    "from Model.modules_dense_nn import DenseNN, PersonalizedScalarNN\n",
    "from Model.dbn import DBNModel, DBNConfig\n",
    "from Model.data import WorkoutDataset, WorkoutDatasetConfig, make_dataloaders\n",
    "from Model.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather(\"../output/endomondo.feather\")\n",
    "df_tmp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def safe_flatten(x):\n",
    "    if isinstance(x, (list, np.ndarray)):  \n",
    "        # Flatten the list or array\n",
    "        if isinstance(x, np.ndarray) and x.ndim > 1:\n",
    "            return x.flatten().tolist()\n",
    "        return [item for sublist in x for item in sublist] if isinstance(x, list) else x.tolist()\n",
    "    else:\n",
    "        # Return single float as a list with one item\n",
    "        return [x]\n",
    "\n",
    "# Apply the function to the column\n",
    "y = df['heart_rate_normalized'].apply(safe_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list-like columns into individual columns per feature if necessary.\n",
    "X = df[['speed_h', 'speed_v', 'distance']].apply(lambda x: np.concatenate(x.values).ravel(), axis=1)\n",
    "y = df['heart_rate_normalized'].apply(lambda x: np.concatenate([x]).ravel() if isinstance(x, list) else x)\n",
    "\n",
    "# Check if the number of samples matches\n",
    "assert len(X) == len(y), \"Mismatched X and y lengths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30658,) (30658,)\n",
      "(7665,) (7665,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30430/30430 [00:09<00:00, 3290.06it/s]\n",
      "100%|██████████| 30430/30430 [00:02<00:00, 11183.72it/s]\n",
      "100%|██████████| 38323/38323 [00:05<00:00, 6765.97it/s]\n",
      "100%|██████████| 38323/38323 [00:11<00:00, 3437.22it/s]\n"
     ]
    }
   ],
   "source": [
    "data_config_train = WorkoutDatasetConfig(\n",
    "    subject_id_column = \"userId\",\n",
    "    workout_id_column = \"id\",\n",
    "    time_since_start_column ='time_grid',\n",
    "    time_of_start_column = 'start_dt',\n",
    "    heart_rate_column = 'heart_rate',\n",
    "    heart_rate_normalized_column = 'heart_rate_normalized',\n",
    "    activity_columns = [\"speed_h\", \"speed_v\"],\n",
    "    weather_columns = [],\n",
    "    history_max_length=512,   \n",
    ")\n",
    "data_config_test = dataclasses.replace(data_config_train, chunk_size=None, stride=None)\n",
    "\n",
    "train_dataset = WorkoutDataset(df_tmp[df_tmp[\"in_train\"]], data_config_train)\n",
    "test_dataset = WorkoutDataset(df_tmp, data_config_test)\n",
    "\n",
    "train_dataloader, test_dataloader = make_dataloaders(train_dataset, test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# Load the LSTM model (DBNModel in your case)\n",
    "# Assuming the model has already been defined and trained\n",
    "model = DBNModel(config=dbn_config, workouts_info=df_tmp[[\"userId\", \"id\"]])\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Check that the model is ready\n",
    "print(\"Model loaded and ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBNModel(\n",
       "  (embedding_store): EmbeddingStore(\n",
       "    (subject_embeddings): Embedding(558, 8, max_norm=5.0)\n",
       "    (encoder): LSTMEncoder(\n",
       "      (lstm): LSTM(5, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (fc): Linear(in_features=256, out_features=8, bias=True)\n",
       "      (batch_norm): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (lstm_encoder): LSTMEncoder(\n",
       "    (lstm): LSTM(5, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (batch_norm): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adafs_soft): AdaFSSoft(\n",
       "    (controller): ControllerMLP(\n",
       "      (mlp): MultiLayerPerceptron(\n",
       "        (mlps): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=1216, out_features=1216, bias=True)\n",
       "            (1): BatchNorm1d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_model): TransitionModel(\n",
       "    (lstm): LSTM(19, 256, batch_first=True)\n",
       "    (fc): Linear(in_features=256, out_features=8, bias=True)\n",
       "  )\n",
       "  (emission_model): EmissionModel(\n",
       "    (fc): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       "  (A): PersonalizedScalarNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (output_activation): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       "  (B): PersonalizedScalarNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (output_activation): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       "  (alpha): PersonalizedScalarNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (output_activation): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       "  (beta): PersonalizedScalarNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (output_activation): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       "  (hr_min): PersonalizedScalarNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (output_activation): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       "  (hr_max): PersonalizedScalarNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=12, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (output_activation): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Model.dbn import DBNModel, DBNConfig\n",
    "from Model.trainer import Trainer\n",
    "\n",
    "# Define Model Configuration\n",
    "dbn_config = DBNConfig(\n",
    "    data_config=data_config_train,\n",
    "    seq_length=64, \n",
    "    learning_rate=1e-3,\n",
    "    seed=0,\n",
    "    n_epochs=10,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_layers=2,\n",
    "    dbn_hidden_dim=64,\n",
    "    personalization=\"none\",\n",
    "    dim_personalization=8,\n",
    "    subject_embedding_dim=8,\n",
    "    encoder_embedding_dim=8,\n",
    "    dropout=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Instantiate the Model\n",
    "model = DBNModel(\n",
    "    config=dbn_config,\n",
    "    workouts_info=df_tmp[[\"userId\", \"id\"]]\n",
    ")\n",
    "model\n",
    "# Load the trained state dictionary into the model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Your model is now ready for evaluation or inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7665, 2158])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assuming X_test is a list of numpy arrays (or lists)\n",
    "X_test_tensors = [torch.tensor(x, dtype=torch.float32) for x in X_test.values]\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "X_test_padded = pad_sequence(X_test_tensors, batch_first=True)\n",
    "\n",
    "# Ensure padding was successful\n",
    "print(X_test_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m workout_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkout_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkout_id\u001b[39m\u001b[38;5;124m'\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkout_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubject_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkout_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkout_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Collect predictions and move to CPU for further processing\u001b[39;00m\n\u001b[0;32m     21\u001b[0m all_predictions\u001b[38;5;241m.\u001b[39mappend(predictions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\hyston\\AppData\\Local\\anaconda3\\envs\\experiments\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hyston\\AppData\\Local\\anaconda3\\envs\\experiments\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Final_code\\Model\\dbn.py:182\u001b[0m, in \u001b[0;36mDBNModel.forward\u001b[1;34m(self, workout_ids, activity, history, subject_ids)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, workout_ids, activity, history, subject_ids):\n\u001b[0;32m    181\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_store\u001b[38;5;241m.\u001b[39mget_embeddings_from_workout_ids(workout_ids, history)\n\u001b[1;32m--> 182\u001b[0m     lstm_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_encoder(history)\n\u001b[0;32m    183\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([embeddings, lstm_output, activity], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    184\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m combined_features\u001b[38;5;241m.\u001b[39mview(combined_features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Ensure correct shape\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Ensure that the model is on the correct device\n",
    "device = dbn_config.device  # This is likely set to \"cuda\" or \"cuda:0\" if a GPU is available\n",
    "\n",
    "# Move model to the correct device if it's not already there\n",
    "model.to(device)\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Extract the necessary inputs from the batch dictionary and convert to tensor if needed, then move to the correct device\n",
    "        activity = torch.tensor(batch['activity']).to(device) if isinstance(batch['activity'], np.ndarray) else batch['activity'].to(device)\n",
    "        history = torch.tensor(batch['history']).to(device) if isinstance(batch['history'], np.ndarray) else batch['history'].to(device)\n",
    "        subject_ids = torch.tensor(batch['subject_id']).to(device) if isinstance(batch['subject_id'], np.ndarray) else batch['subject_id'].to(device)\n",
    "        workout_ids = torch.tensor(batch['workout_id']).to(device) if isinstance(batch['workout_id'], np.ndarray) else batch['workout_id'].to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        predictions = model(activity=activity, history=history, subject_ids=subject_ids, workout_ids=workout_ids)\n",
    "        \n",
    "        # Collect predictions and move to CPU for further processing\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "# Combine all batch predictions into one array\n",
    "predicted_heart_rates = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "print(predicted_heart_rates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject_id': array([3905196, 3905196, 3905196, 3905196, 3905196,  653747,  653747,\n",
      "        653747,  653747,  653747,  653747, 1609501, 1342020,   81753,\n",
      "        653747,  653747,  260784,  260784,  136171, 2390403, 2390403,\n",
      "       2390403, 2390403, 2390403, 2390403, 2390403, 2390403, 2390403,\n",
      "       2390403, 2390403, 2390403, 2390403, 2390403, 2390403, 3471841,\n",
      "       2390403, 2390403, 2390403, 3471841, 2390403, 3471841, 2390403,\n",
      "        804068, 2390403, 2390403, 2390403, 2390403, 2390403, 2390403,\n",
      "       2390403, 2390403, 2390403, 2390403, 2390403, 2390403, 2390403,\n",
      "         81753, 2390403,   81753, 2390403, 2175400, 2175400, 2390403,\n",
      "       4025656, 2390403, 4025656, 1818822, 2390403, 2390403, 1818822,\n",
      "       1732973, 4025656, 2390403, 2390403, 2390403, 1732973, 2390403,\n",
      "       8582302, 2390403, 8582302, 8582302, 4025656, 2175400, 2390403,\n",
      "       8582302, 8582302, 2390403, 1818822, 2175400, 1818822, 2175400,\n",
      "       1818822, 2232554, 1818822, 2175400, 4025656, 1818822, 2390403,\n",
      "       2390403, 2175400, 1818822, 2175400, 2390403, 1342020, 2175400,\n",
      "       2175400, 1342020, 2390403, 1290773, 4025656, 1290773, 1290773,\n",
      "       1342020, 1023513, 1818822, 2175400, 4025656, 1290773,  336611,\n",
      "       1023513, 2390403, 1290773, 1023513, 2390403, 2390403, 1290773,\n",
      "       1023513, 2390403], dtype=int64), 'workout_id': array([ 40328735,  40328733,  40328592,  40328597,  40328871, 562750273,\n",
      "       472711698, 472713541, 472719123,  28560724, 445387800,  11785115,\n",
      "       415283119,    728124, 445560626,   4587711,  75650539,  75650590,\n",
      "         4661231, 169742231, 169810870, 169810915, 169811074, 169811081,\n",
      "       169811111, 169811143, 169811418, 169811456, 169811497, 169811616,\n",
      "       169811766, 169811807, 169811855, 169811893, 266101356, 169811930,\n",
      "       169811962, 169812003, 266101329, 169812038, 266101303, 169812078,\n",
      "         6881832, 169812157, 169812186, 169812215, 169812248, 169812363,\n",
      "       169812448, 169812486, 169812524, 169812603, 169812684, 169812716,\n",
      "       169812755, 169812796,   8590081, 169812913,   9104747, 169813063,\n",
      "        18563444,  18562926, 169813139,  44932865, 169813198,  44932856,\n",
      "        22278990, 169813230, 169813272,  22278975,  13698113,  44932847,\n",
      "       169813352, 169813395, 169813430,  13698078, 169813500, 378205011,\n",
      "       169813529, 378204814, 378204563,  44932836,  19215305, 169813615,\n",
      "       378203811, 378203497, 169813651,  22278967,  18994860,  22278962,\n",
      "        18994829,  22278960,  19433331,  22278949,  18994657,  44932817,\n",
      "        22278938, 169813741, 169813781,  18977721,  22278933,  18977696,\n",
      "       169813823,  14610440,  18781588,  18781558,  14610433, 169813887,\n",
      "        14371394,  44932794,  14559453,  14615469,  14736511, 539549684,\n",
      "        22278925,  18709037,  44932791,  14919797, 494938790, 539549510,\n",
      "       169813945,  15063276, 539548942, 170094540, 170094542,  15432462,\n",
      "       539547797, 170094858], dtype=int64), 'heart_rate': tensor([[ 99.5000, 125.0000, 132.2500,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [107.0000, 128.5000, 135.8462,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [ 90.3000, 108.1250, 117.2222,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        ...,\n",
      "        [ 89.2500,  98.0000, 105.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [104.0000, 111.0000, 114.7500,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [ 91.1111, 103.0000, 110.8000,  ...,   0.0000,   0.0000,   0.0000]]), 'activity': tensor([[[ 5.4938,  1.2017],\n",
      "         [ 3.3164,  0.9133],\n",
      "         [ 4.1905,  0.9372],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[ 3.2657,  0.5767],\n",
      "         [ 2.7350,  0.0120],\n",
      "         [ 2.8968,  0.1654],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[ 1.2868,  0.0865],\n",
      "         [ 3.1787, -0.0264],\n",
      "         [ 2.6392, -0.0814],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.8722, -0.1100],\n",
      "         [ 2.7012, -0.1300],\n",
      "         [ 3.3338, -0.1600],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[ 1.7163, -0.4000],\n",
      "         [ 1.8886, -0.3200],\n",
      "         [ 1.8529, -0.2100],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[ 2.3644,  0.0289],\n",
      "         [ 2.5508,  0.3111],\n",
      "         [ 2.8535,  0.5160],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]]]), 'time': tensor([[0.0000, 0.0083, 0.0167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0083, 0.0167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0083, 0.0167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0083, 0.0167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0083, 0.0167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0083, 0.0167,  ..., 0.0000, 0.0000, 0.0000]]), 'weather': tensor([], size=(128, 0)), 'full_workout_length': tensor([300, 288, 291, 301, 216, 315, 644, 241, 184, 383, 490, 252, 345, 158,\n",
      "        414, 411, 160, 174, 179, 291, 258, 219, 159, 321, 236, 339, 288, 335,\n",
      "        389, 360, 329, 342, 454, 375, 228, 431, 299, 361, 210, 421, 141, 585,\n",
      "         94, 502, 330, 486, 289, 240, 386, 508, 389, 225, 428, 594, 339, 513,\n",
      "        424, 462, 103, 402, 393, 235, 274, 307, 273, 135, 185, 411, 339, 240,\n",
      "        280, 286, 299, 439, 296, 364, 250, 144, 422, 134, 262, 194, 336, 205,\n",
      "        193, 249, 333, 247, 301, 471, 158, 253, 639, 419, 222, 229, 327,  93,\n",
      "        235, 315, 183, 495, 299, 194, 302, 223, 229, 325, 305, 224, 223, 422,\n",
      "        372, 181, 336, 302, 231, 151, 119, 180, 276, 284, 240, 286, 292, 252,\n",
      "        270, 322]), 'history': tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.9318,  0.0000,  5.4938,  1.2017,  1.0174],\n",
      "         [-0.7727,  0.0083,  3.3164,  0.9133,  1.0174],\n",
      "         [-0.4432,  0.0167,  4.1905,  0.9372,  1.0174],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.6364,  0.6333,  2.9226, -0.0739,  3.0829],\n",
      "         [ 0.6364,  0.6417,  3.1770,  0.1202,  3.0829],\n",
      "         [ 0.5519,  0.6500,  3.0327, -0.0241,  3.0829],\n",
      "         ...,\n",
      "         [ 0.9727,  2.3750,  4.2108, -0.0961,  2.9985],\n",
      "         [ 1.1545,  2.3833,  3.2555, -0.1602,  2.9985],\n",
      "         [ 1.3364,  2.3917,  3.2555, -0.1602,  2.9985]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4545,  2.8750,  2.5296,  0.2067,  2.3248],\n",
      "         [ 0.4545,  2.8833,  2.5939, -0.0900,  2.3248],\n",
      "         [ 0.4773,  2.8917,  2.6402,  0.1000,  2.3248],\n",
      "         ...,\n",
      "         [ 0.6500,  2.3417,  3.1070,  0.0207,  1.6240],\n",
      "         [ 0.6818,  2.3500,  3.4609,  0.0760,  1.6240],\n",
      "         [ 0.6364,  2.3583,  2.4966, -0.2200,  1.6240]],\n",
      "\n",
      "        [[ 0.9091,  0.7417,  1.9167, -0.0333,  2.2020],\n",
      "         [ 0.9091,  0.7500,  2.0087, -0.0400,  2.2020],\n",
      "         [ 0.8636,  0.7583,  1.9608, -0.0300,  2.2020],\n",
      "         ...,\n",
      "         [ 1.9318,  1.9750,  0.1447, -0.0200,  1.1113],\n",
      "         [ 1.8636,  1.9833,  0.9243,  0.0200,  1.1113],\n",
      "         [ 1.8977,  1.9917,  2.8393, -0.0450,  1.1113]],\n",
      "\n",
      "        [[ 0.7727,  0.5500,  3.0999, -0.1636,  1.1092],\n",
      "         [ 0.7727,  0.5583,  2.8365,  0.0400,  1.1092],\n",
      "         [ 0.8750,  0.5667,  2.9987,  0.0850,  1.1092],\n",
      "         ...,\n",
      "         [ 1.4091,  2.4083,  2.7115, -0.3000,  0.7129],\n",
      "         [ 1.3409,  2.4167,  2.4320,  0.2700,  0.7129],\n",
      "         [ 1.2727,  2.4250,  2.5192,  0.3100,  0.7129]]]), 'activity_measurements_names': ['speed_h', 'speed_v'], 'weather_measurements_names': [], 'history_length': tensor([  1, 300, 512, 512, 512,   1, 315, 512, 512, 512, 512,   1,   1,   1,\n",
      "        512, 512,   1, 160,   1,   1, 291, 512, 512, 512, 512, 512, 512, 512,\n",
      "        512, 512, 512, 512, 512, 512,   1, 512, 512, 512, 228, 512, 438, 512,\n",
      "          1, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512,\n",
      "        158, 512, 512, 512,   1, 393, 512,   1, 512, 307,   1, 512, 512, 185,\n",
      "          1, 442, 512, 512, 512, 280, 512,   1, 512, 144, 278, 512, 512, 512,\n",
      "        512, 512, 512, 425, 512, 512, 512, 512,   1, 512, 512, 512, 512, 512,\n",
      "        512, 512, 512, 512, 512, 345, 512, 512, 512, 512,   1, 512, 305, 512,\n",
      "        512,   1, 512, 512, 512, 512,   1, 181, 512, 512, 361, 512, 512, 512,\n",
      "        512, 512])}\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    print(batch)\n",
    "    break  # Print just the first batch to check the structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_heart_rates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recommendations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Example: Generate recommendations for the first test case\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m recs \u001b[38;5;241m=\u001b[39m recommend_workouts(\u001b[43mpredicted_heart_rates\u001b[49m[\u001b[38;5;241m0\u001b[39m], df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_heart_rates' is not defined"
     ]
    }
   ],
   "source": [
    "def recommend_workouts(predicted_hr, df, threshold=10):\n",
    "    recommendations = []\n",
    "    for i, workout in df.iterrows():\n",
    "        workout_predicted_hr = model(torch.tensor([workout['speed_h'], workout['speed_v'], workout['distance']], dtype=torch.float32)).item()\n",
    "        if abs(workout_predicted_hr - predicted_hr) < threshold:\n",
    "            recommendations.append(workout)\n",
    "    return recommendations\n",
    "\n",
    "# Example: Generate recommendations for the first test case\n",
    "recs = recommend_workouts(predicted_heart_rates[0], df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# For simplicity, let's evaluate on the first few test cases\n",
    "mae = mean_absolute_error([item for sublist in y_test[:5] for item in sublist], predicted_heart_rates[:5])\n",
    "rmse = mean_squared_error([item for sublist in y_test[:5] for item in sublist], predicted_heart_rates[:5], squared=False)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot actual vs predicted heart rates for the first test case\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([item for sublist in y_test[:1] for item in sublist], label='Actual Heart Rates', marker='o')\n",
    "plt.plot(predicted_heart_rates[:1], label='Predicted Heart Rates', marker='x')\n",
    "plt.title('Actual vs Predicted Heart Rates')\n",
    "plt.xlabel('Time Point Index')\n",
    "plt.ylabel('Heart Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Exp-py38",
   "language": "python",
   "name": "experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
